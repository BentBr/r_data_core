# r_data_core - Master Data Management Service

## Project Overview
r_data_core is a Rust-based MDM (Master Data Management) service that provides custom entity definition capabilities. It allows users to create custom entities (like users, customers, products, categories) that can be queried and managed through automatically generated REST endpoints.

## Core Concepts

### Entities
- Custom objects that users can create and define
- Examples: users, customers, products, categories, technical objects
- Have standard fields (uuid, creator_id, created_at, updated_at, etc.)
- Custom fields, validation, and definitions are set via the Entity Definition API
- Support dynamic schema evolution
- **Auto-created Views**: When an entity definition is created or updated, the system automatically:
  - Creates a table `entity_{entity_type}` for storing entity-specific field data
  - Creates a view `entity_{entity_type}_view` that joins `entities_registry` (metadata) with the entity-specific table
  - The view provides a unified interface for querying entities with both registry metadata and custom fields
  - INSTEAD OF triggers on the view handle INSERT and UPDATE operations transparently
  - Views are automatically updated when entity definitions change

### Entity Definitions
- Define the structure and validation rules for entities
- Specify field types, constraints, and UI configurations
- Support complex field types and relationships
- Managed through the admin API

### Workflows
- Automated data processing pipelines (Consumer and Provider workflows)
- Support scheduled execution via cron expressions
- DSL-based workflow definitions for data transformation
- Workflow runs with job queue system
- Support for data import/export, transformations, and entity operations
- Managed through the admin API

### Versioning
- Automatic versioning for entities, entity definitions, and workflows
- Append-only snapshot system for tracking changes over time
- Version history with creator tracking and timestamps
- Ability to view and retrieve historical versions
- Configurable versioning (can be disabled per workflow)

## API Architecture

### Public API (`/api/docs/`)
- **Authentication**: JWT token (from admin login) or API key
- **Purpose**: Access to all entity types created via entity definitions
- **Features**: 
  - Full CRUD operations
  - Advanced filtering and pagination
  - Dynamic entity creation and management
  - Query capabilities with complex filters

### Admin API (`/admin/api/docs/`)
- **Authentication**: Admin JWT authentication only
- **Purpose**: System administration and configuration
- **Features**:
  - Admin user management
  - Entity definition creation and management
  - API key management
  - System configuration
  - Permissions and workflows
  - Workflow management (create, update, delete, run workflows)
  - Version history access (entities, entity definitions, workflows)

## Technology Stack

### Backend
- **Language**: Rust
- **Framework**: Actix-web
- **Database**: PostgresSQL with advanced features
- **Caching**: Redis
- **Authentication**: JWT-based
- **API Documentation**: OpenAPI/Nelmio Swagger

### Development & Deployment
- **Containerization**: Docker & Docker Compose
- **Target**: Kubernetes-ready container images
- **Database**: PostgresSQL with migrations
- **Cache**: Redis for performance optimization

### Frontend
- **Language**: Vue3, TypeScript
- **Framework**: Vite
- **UI**: Vuetify

## Project Structure

### Key Directories
- `src/api/` - API endpoints and routing
- `src/entity/` - Core entity and entity definition logic
- `src/services/` - Business logic and service layer
- `src/db/` - Database operations and repositories
- `src/workflow/` - Workflow management
- `src/versioning/` - Version management
- `src/bin/` - Binary executables (main app, workers, utilities)
- `tests/` - Integration and unit tests
- `migrations/` - Database schema migrations
- `fe` - our Vue3 frontend

### Important Modules
- `entity/dynamic_entity/` - Dynamic entity implementation
- `entity/entity_definition/` - Entity definition system
- `workflow/` - Workflow engine with DSL and job queue
- `versioning/` - Version management service
- `entity/version_repository.rs` - Entity versioning repository
- `api/admin/` - Administrative endpoints (includes workflows, entity definitions with versioning)
- `api/public/` - Public entity endpoints
- `services/` - Business logic services (includes workflow_service, version_service)
- `bin/worker.rs` - Workflow worker (processes workflow jobs from Redis queue)
- `bin/maintenance.rs` - Maintenance worker (scheduled tasks like version pruning)

## Development Guidelines

### Code Quality
- Always run `cargo fmt` after changes
- Follow strict `cargo clippy -- -D clippy::all -D warnings -D clippy::pedantic -D clippy::nursery` standards
- Maintain comprehensive test coverage
- Use proper error handling and logging

### Testing Strategy
- **Unit Tests**: Run with `rdt test-unit` (can run in parallel)
- **Integration Tests**: Run with `rdt test` (sequential execution)
- **FE Unit Tests**: Run with `rdt test-fe`
- **Test Location**: `/tests/` directory for integration tests
- **Test Coverage**: Aim for comprehensive coverage of all modules
- **Important**: Cursor agents should ONLY run `rdt test` and `rdt test-fe` - never run `cargo test` or `npm test` directly
- Never run `cargo test` as this will have failing tests!
- Never run `npm test` as we have a docker container for node

### Database
- Use migrations for schema changes
- Follow naming conventions
- Implement proper indexing strategies
- Handle database connections efficiently

## Current Status

### Implemented Features
- âœ… Dynamic entity system
- âœ… Entity definition API
- âœ… Authentication (JWT + API keys)
- âœ… CRUD operations
- âœ… Advanced filtering and pagination
- âœ… Admin user management
- âœ… Database migrations
- âœ… Caching layer
- âœ… Comprehensive test suite
- âœ… Workflow engine with DSL-based definitions
- âœ… Workflow job queue and execution system
- âœ… Workflow worker for processing scheduled and queued workflows
- âœ… Maintenance worker for automated system maintenance tasks
- âœ… Versioning system for entities, entity definitions, and workflows
- âœ… Version history tracking with creator information

### In Progress / Planned
- ðŸ”„ Notifications system
- ðŸ”„ Advanced permission schemes
- ðŸ”„ Performance optimizations

## Local Development Setup

### Prerequisites
- Rust toolchain
- Docker & Docker Compose
- PostgresSQL (via Docker)
- Redis (via Docker)

### Quick Start
1. Clone the repository
2. Run `docker-compose up -d` for database and Redis
3. Run migrations: `cargo run --bin run_migrations`
4. Start the application: `cargo run`
5. (Optional) Start workers:
   - Workflow worker: `cargo run --bin worker` (processes workflow jobs)
   - Maintenance worker: `cargo run --bin maintenance` (scheduled maintenance tasks)
6. Access APIs at:
   - Public API: `http://rdatacore.docker/api/docs/`
   - Admin API: `http://rdatacore.docker/admin/api/docs/`

## Common Commands

### Development
```bash
# Format code
cargo fmt

# Lint code
cargo clippy -- -D clippy::all -D warnings -D clippy::pedantic -D clippy::nursery

# Run unit tests
rdt test-unit

# Run integration tests
rdt test

# Run integration tests
rdt test-fe

# Apply database migrations
cargo run --bin run_migrations

# Hash passwords (for admin users)
cargo run --bin hash_password

# Start workflow worker (processes workflow jobs from Redis queue)
cargo run --bin worker

# Start maintenance worker (scheduled maintenance tasks)
cargo run --bin maintenance
```

### Docker Operations
```bash
# Start services
docker-compose up -d

# Stop services
docker-compose down

# View logs
docker-compose logs -f
```

## Architecture Notes

### Key Design Principles
- Separation of concerns between API, services, and data layers
- Dynamic entity system for maximum flexibility
- Comprehensive validation and error handling
- Scalable caching strategy
- Secure authentication and authorization

### Performance Considerations
- Efficient database queries with proper indexing
- Redis caching for frequently accessed data
- Connection pooling for database operations
- Async/await patterns for non-blocking operations

### Security Features
- JWT-based authentication
- API key management
- Role-based access control
- Input validation and sanitization
- Secure password hashing

## Configuration

### Environment Variables

#### Application (Main Server) - Mandatory
- `DATABASE_URL` - PostgreSQL connection string (required)
- `JWT_SECRET` - Secret key for JWT token signing (required)
- `REDIS_URL` - Redis connection URL for queue and cache (required)

#### Application (Main Server) - Optional
- `APP_ENV` - Application environment (default: "development")
- `API_HOST` - Server host address (default: "0.0.0.0")
- `API_PORT` - Server port (default: 8888)
- `API_USE_TLS` - Enable SSL/TLS (default: false)
- `JWT_EXPIRATION` - JWT token expiration in seconds (default: 86400)
- `API_ENABLE_DOCS` - Enable API documentation (default: true)
- `CORS_ORIGINS` - Comma-separated list of allowed CORS origins (default: "*")
- `DATABASE_MAX_CONNECTIONS` - Maximum database connections in pool (default: 10)
- `DATABASE_CONNECTION_TIMEOUT` - Connection timeout in seconds (default: 30)
- `LOG_LEVEL` - Logging level: info/debug/error (default: "info")
- `LOG_FILE` - Optional log file path
- `CACHE_ENABLED` - Enable caching (default: true)
- `CACHE_TTL` - Default cache TTL in seconds (default: 300)
- `CACHE_MAX_SIZE` - Maximum cache size in items (default: 10000)
- `CACHE_ENTITY_DEFINITION_TTL` - Entity definition cache TTL in seconds, 0 = infinite (default: 0)
- `CACHE_API_KEY_TTL` - API key cache TTL in seconds (default: 600)
- `QUEUE_FETCH_KEY` - Redis key for fetch jobs queue (default: "queue:workflows:fetch")
- `QUEUE_PROCESS_KEY` - Redis key for process jobs queue (default: "queue:workflows:process")

#### Workflow Worker - Mandatory
- `WORKER_DATABASE_URL` - PostgreSQL connection string for worker (required)
- `REDIS_URL` - Redis connection URL for queue (required)
- `JOB_QUEUE_UPDATE_INTERVAL` - Interval in seconds to reconcile scheduled jobs with DB (required, must be > 0)

#### Workflow Worker - Optional
- `WORKER_DATABASE_MAX_CONNECTIONS` - Maximum database connections (default: 10)
- `DATABASE_CONNECTION_TIMEOUT` - Connection timeout in seconds (default: 30)
- `WORKFLOW_WORKER_THREADS` - Number of worker threads (default: 4)
- `WORKFLOW_DEFAULT_TIMEOUT` - Default workflow timeout in seconds (default: 300)
- `WORKFLOW_MAX_CONCURRENT` - Maximum concurrent workflows (default: 10)
- `QUEUE_FETCH_KEY` - Redis key for fetch jobs queue (default: "queue:workflows:fetch")
- `QUEUE_PROCESS_KEY` - Redis key for process jobs queue (default: "queue:workflows:process")
- `CACHE_ENABLED` - Enable caching (default: true)
- `CACHE_TTL` - Default cache TTL in seconds (default: 300)
- `CACHE_ENTITY_DEFINITION_TTL` - Entity definition cache TTL (default: 0 = infinite)
- `CACHE_API_KEY_TTL` - API key cache TTL in seconds (default: 600)

#### Maintenance Worker - Mandatory
- `MAINTENANCE_DATABASE_URL` - PostgreSQL connection string for maintenance worker (required)
- `REDIS_URL` - Redis connection URL for cache (required)

#### Maintenance Worker - Optional
- `MAINTENANCE_CRON` - Cron expression for maintenance scheduler (default: "*/5 * * * *")
- `MAINTENANCE_DATABASE_MAX_CONNECTIONS` - Maximum database connections (default: 10)
- `DATABASE_CONNECTION_TIMEOUT` - Connection timeout in seconds (default: 30)
- `CACHE_ENABLED` - Enable caching (default: true)
- `CACHE_TTL` - Default cache TTL in seconds (default: 300)

## Queue System (Apalis)

The system uses **Apalis** with Redis for workflow job queuing:

### How It Works
- **Redis Lists**: Uses Redis Lists (RPUSH/BLPOP) for queue operations
- **Two Queues**:
  - `fetch` queue: Jobs for fetching and staging data from external sources
  - `process` queue: Jobs for processing staged items (transformations, imports, exports)
- **Blocking Operations**: Workers use `BLPOP` to block until jobs are available
- **Job Serialization**: Jobs are serialized as JSON and stored in Redis

### Workflow Execution Flow
1. **Scheduled Workflows**: Workflow worker scans for enabled workflows with cron schedules
2. **Manual Triggers**: Workflows can be triggered via API
3. **Job Enqueueing**: Fetch jobs are enqueued to Redis `fetch` queue
4. **Worker Processing**: Worker pops jobs from queue, creates workflow runs, and processes them
5. **Staging**: Data is fetched from sources and staged in `workflow_raw_items` table
6. **Processing**: Staged items are processed according to workflow DSL configuration
7. **Completion**: Workflow runs are marked as success/failure with statistics

### Maintenance Tasks

The maintenance worker runs scheduled tasks based on cron expression:

- **Entity Version Pruning**: 
  - Prunes old entity versions based on system settings
  - Two pruning strategies:
    - **By Age**: Removes versions older than configured days (`max_age_days`)
    - **By Count**: Keeps only the latest N versions per entity (`max_versions`)
  - Pruning respects entity versioning settings (can be disabled globally)
  - Settings are configurable via system settings API

